{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b737fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bandit_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebbc337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example list of different model names - different names (RW, HMM) and augmentations (bias, RP - will be used to pick the corresponding model class and add extra augmentations)\n",
    "#possible model: random, WSLS, choice_kernel, RW, PH, HMM\n",
    "#possible augmentations: RP, bias, sticky, forgetting .. - explained in models.py \n",
    "\n",
    "models = [ \n",
    "    'RW_RP_bias', 'RW_RP_sticky', 'RW_RP_bias_sticky',\n",
    "    'HMM_RP_bias', 'HMM_RP_fixed_aur','HMM_RP_fixed_aur_bias', \n",
    "    'HMM_RP_bias_fixed_beta', 'HMM_RP_fixed_aur_fixed_beta','HMM_RP_fixed_aur_bias_fixed_beta',\n",
    "    'HMM_RP_bias_sticky', 'HMM_RP_fixed_aur_sticky','HMM_RP_fixed_aur_bias_sticky', \n",
    "    'HMM_RP_bias_fixed_beta_sticky', 'HMM_RP_fixed_aur_fixed_beta_sticky','HMM_RP_fixed_aur_bias_fixed_beta_sticky'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44f3f1",
   "metadata": {},
   "source": [
    "example from models.py how different augmentations added to RW :  \n",
    "\n",
    "        if 'RP' in model: self.RP = True #reward-punishment = different alphas for rewarded/unrewarded trials \n",
    "        if 'DSA' in model: self.DSA = True #double updating - counterfactual updating of values for unchosen option\n",
    "        if 'same_alpha' in model: self.same_alpha = True #for DSA - whether use same or different alpha for counterfactual update \n",
    "        if 'scale_alpha' in model: self.scale_alpha = True #for DSA - can get alpha_c by scaling alpha \n",
    "                \n",
    "        if 'forgetting' in model: self.forgetting = True \n",
    "        if 'bias' in model: self.go_bias = True\n",
    "        if 'sensitivity' in model: self.sensitivity = True #scaling of reward by rho - (0-1)\n",
    "        if 'sticky' in model: self.sticky = True #addition of a choice kernel \n",
    "        if 'repeat' in model: self.repeat = True #increase tendency to repeat the choice \n",
    "        \n",
    "        #forgetting options \n",
    "        if 'reset_to_mean' in model: self.reset_to_mean = True #one trial reset of unchosen option to mean\n",
    "        if 'reset_to_neutral' in model: self.reset_to_neutral = True #one trial reset of unchosen option to 0.5\n",
    "        if 'forget_to_mean' in model: self.forget_to_mean = True #gradual decay bound at 0.5 \n",
    "        if 'forget_to_neutral' in model: self.forget_to_neutral = True #gradual decay bound at mean \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97702b0e",
   "metadata": {},
   "source": [
    "same for HMM:\n",
    "        if 'RP' in model: self.RP = True #reward-punishment = different alphas for rewarded/unrewarded trials \n",
    "        if 'bias' in model: self.bias = True\n",
    "        if 'sticky' in model: self.sticky = True  #addition of a choice kernel \n",
    "        if 'fixed_beta' in model: self.fixed_beta = True  #version with fixed beta as suggested in quentin's paper \n",
    "        if 'fixed_gamma' in model: self.fixed_gamma = True #for the version with fixed transition matrix \n",
    "        if 'fixed_aur' in model: self.fixed_aur = True #in previous fits alpha_ur (d) often came out at the lowest value close to 0 - this version fixes alpha_ur at 0 from the start - so no updating at reward omission\n",
    "        if 'repeat' in model: self.repeat = True #increase tendency to repeat the choice \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da41b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models classes and fitting\n",
    "#fitting takes pandas data frame with choices in column 'right_or_left' - 0 or 1; rewards in 'reward' - 0 or 1; \n",
    "#fit each session separately; splits different conditions per mouse, if only one condition - \n",
    "\n",
    "\n",
    "fit_df = pd.DataFrame()\n",
    "#based on the model name - pick the model class to run fits \n",
    "for model in models:\n",
    "    if 'random' in model:\n",
    "        fit = random_fit(data,model)\n",
    "        temp = fit.fit_data()\n",
    "        fit_df =pd.concat([fit_df,temp])\n",
    "    \n",
    "    if 'WSLS' in model:\n",
    "        fit = WSLS_fit(data,model)\n",
    "        temp = fit.fit_data()\n",
    "        fit_df =pd.concat([fit_df,temp])\n",
    "    \n",
    "    if 'choice_kernel' in model:\n",
    "        fit = CK_fit(data,model)\n",
    "        temp = fit.fit_data()\n",
    "        fit_df =pd.concat([fit_df,temp])\n",
    "\n",
    "    if 'RW' in model:\n",
    "        fit = RW_fit(data,model)\n",
    "        temp = fit.fit_data()\n",
    "        fit_df =pd.concat([fit_df,temp])\n",
    "    \n",
    "    if 'HMM' in model:\n",
    "        fit = HMM_fit(data,model)\n",
    "        temp = fit.fit_data()\n",
    "        fit_df =pd.concat([fit_df,temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b65b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example simulations\n",
    "to_sim = ['RW','RW_forgetting','HMM_RP_fixed_aur_fixed_beta','RW_DSA_same_alpha_RP_bias']\n",
    "\n",
    "sims = pd.DataFrame()\n",
    "\n",
    "\n",
    "sims = pd.DataFrame()\n",
    "for model in to_sim:\n",
    "    if 'RW' in model:\n",
    "        #define model\n",
    "        sim = RW_sim(model = model,fit_df = data_fits[data_fits.model==model],from_fit = True)#use parameters from fits \n",
    "        #run simulation \n",
    "        temp = sim.simulate(runs = 50,track_var = True) #track_var - tracks values and RPEs \n",
    "        sims =pd.concat([sims,temp]).reset_index(drop = True)\n",
    "            \n",
    "    if 'HMM' in model:\n",
    "        sim = HMM_sim(model = model,fit_df = data_fits[data_fits.model==model],from_fit = True)\n",
    "        temp = sim.simulate(runs = 50,track_var = True)\n",
    "        temp['runs'] = i\n",
    "        sims =pd.concat([sims,temp]).reset_index(drop = True)\n",
    "        \n",
    "sims['condition'] = 'sim'\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
